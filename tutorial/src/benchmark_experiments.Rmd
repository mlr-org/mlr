# Benchmark Experiments

In a benchmark experiment different learning methods are applied to one or several data sets
with the aim to compare and rank the algorithms with respect to one or more
performance measures.

In [%mlr] a benchmark experiment can be conducted by calling function [&benchmark] on
a [list](&base::list) of [Learner](&makeLearner)s and a [list](&base::list) of [&Task]s.
[&benchmark] basically executes [&resample] for each combination of [Learner](&makeLearner)
and [&Task].
You can specify an individual resampling strategy for each [&Task] and select one or
multiple performance measures to be calculated.


## Example: One task, two learners, prediction on a single test set

We start with a small example. Two learners, [linear discriminant analysis](&MASS::lda) and
[classification trees](&rpart::rpart), are applied to one classification problem ([&sonar.task]).
As resampling strategy we choose `"Holdout"`.
The performance is thus calculated on one single randomly sampled test data set.

In the example below we create a resample description ([ResampleDesc](&makeResampleDesc)),
which is automatically instantiated by [&benchmark].
The instantiation is done only once, that is, the same resample instance
([ResampleInstance](&makeResampleInstance)) is used for all learners applied to the [&Task].
It's also possible to directly pass a [ResampleInstance](&makeResampleInstance).

If you would like to use a *fixed test data set* instead of a randomly selected one, you can
create a suitable [ResampleInstance](&makeResampleInstance) through function
[&makeFixedHoldoutInstance].

```{r}
## Two learners to be compared
lrns = list(makeLearner("classif.lda"), makeLearner("classif.rpart"))

## Choose the resampling strategy
rdesc = makeResampleDesc("Holdout")

## Conduct the benchmark experiment
res = benchmark(lrns, sonar.task, rdesc)

res
```

In the printed table every row corresponds to one pair of [&Task] and [Learner](&makeLearner).
The entries show the mean misclassification error ([mmce](measures.md)), the default performance
measure for classification, on the test data set.

The result `res` is an object of class [&BenchmarkResult]. Basically, this is a [list](&base::list)
of lists of [&ResampleResult] objects, first ordered by [&Task] and then by [Learner](&makeLearner).

[%mlr] provides several accessor functions, named `getBMR<what_to_extract>`, that permit
to retrieve information for further analyses. This includes for example the performances
or predictions of the learning algorithms under consideration.

Let's have a look at the benchmark result above.
[&getBMRPerformances] returns individual performances in resampling runs, while
[&getBMRAggrPerformances] gives the aggregated values.

```{r}
getBMRPerformances(res)

getBMRAggrPerformances(res)
```

Since we used holdout as resampling strategy, individual and aggregated performances are the
same.

Often it is more convenient to work with [data.frame](&base::data.frame)s. You can easily
convert the result structure by setting `as.df = TRUE`.

```{r}
getBMRPerformances(res, as.df = TRUE)

getBMRAggrPerformances(res, as.df = TRUE)
```

Function [&getBMRPredictions] returns the predictions.
Per default, you get a [list](&base::list) of lists of [&ResamplePrediction] objects.
In most cases you might prefer the [data.frame](&base::data.frame) version.

```{r}
getBMRPredictions(res)

head(getBMRPredictions(res, as.df = TRUE))
```

It is also easily possible to access results for certain learners or tasks via their
IDs. Nearly all "getter" functions have a `learner.ids` and a `task.ids` argument.

```{r}
head(getBMRPredictions(res, learner.ids = "classif.rpart", as.df = TRUE))
```

As you might recall, you can use the `id` option in [&makeLearner] or function [&setId]
to set the ID of a [Learner](&makeLearner) and the `id` option of [make*Task](&Task) for
[&Task] IDs.

The IDs of all [Learner](&makeLearner)s and [&Task]s in a benchmark experiment can be retrieved
as follows:

```{r}
getBMRTaskIds(res)

getBMRLearnerIds(res)
```


## Example: Two tasks, three learners, bootstrapping

Let's have a look at a larger benchmark experiment with two classification tasks ([&pid.task]
and [&sonar.task]) and three learning algorithms.
Since the default learner IDs are a little long, we choose shorter names.

For both tasks bootstrapping with 20 iterations is chosen as resampling strategy.
This is achieved by passing a single resample description to [&benchmark], which is then
instantiated automatically once for each [&Task].
Thus, the same instance is used for all learners applied to one task.

It's also possible to choose a different resampling strategy for each [&Task] by passing a
[list](&base::list) of the same length as the number of tasks that can contain both
[ResampleDesc](&makeResampleDesc)s and [ResampleInstance](&makeResampleInstance)s.

In the example below the accuracy ([acc](measures.md)) and the area under curve ([auc](measures.md))
are calculated.

```{r}
## Three learners to be compared
lrns = list(makeLearner("classif.lda", predict.type = "prob", id = "lda"),
  makeLearner("classif.rpart", predict.type = "prob", id = "rpart"),
  makeLearner("classif.randomForest", predict.type = "prob", id = "rF"))

## Two classification tasks
tasks = list(pid.task, sonar.task)

## Use bootstrapping for both tasks
rdesc = makeResampleDesc("Bootstrap", iters = 20)

## Conduct the benchmark experiment
res = benchmark(lrns, tasks, rdesc, measures = list(acc, auc), show.info = FALSE)

res
```

The entries in the printed table show the aggregated accuracies and AUC values.
On the Pima data lda and random forest show nearly identical performance. 
On the sonar example random forest has highest accuracy and AUC.

Instead of just comparing mean performance values it's generally preferable to have a look
at the distribution of performance values obtained in individual resampling runs.
The individual performances on the 20 bootstrap iterations for every task and learner are
retrieved below.

```{r}
perf = getBMRPerformances(res, as.df = TRUE)
head(perf)
```

As part of a first exploratory analysis you might want to create some plots, for example
dotplots, boxplots, densityplots or histograms.
Currently, [%mlr] does not provide any plotting functionality for benchmark experiments.
But based on the [data.frame](&base::data.frame) returned by [&getBMRPerformances] some basic
plots are easily done.

Shown below are boxplots for the accuracy, [acc](measures.md), and densityplots for the AUC,
[auc](measures.md), generated by function [qplot](&ggplot2::qplot) from package
[%ggplot2].

```{r}
qplot(y = acc, x = task.id, colour = learner.id, data = perf, geom = "boxplot")
qplot(auc, colour = learner.id, facets = . ~ task.id, data = perf, geom = "density")
```

In order to plot both performance measures in parallel `perf` is reshaped to long format.
Below we generate grouped boxplots and densityplots for all tasks, learners and measures.

```{r}
perfm = reshape2::melt(perf, id.vars = c("task.id", "learner.id", "iter"), measure.vars = c("acc", "auc"))
head(perfm)

qplot(variable, value, data = perfm, colour = learner.id, facets = . ~ task.id, geom = "boxplot",
  xlab = "measure", ylab = "performance")
qplot(value, data = perfm, colour = learner.id, facets = variable ~ task.id, geom = "density",
  xlab = "performance")
```

It might also be useful to assess if learner performances in single resampling iterations,
i.e., on the same bootstrap sample, are related.
This might help to gain further insight, for example by having a closer look at bootstrap
samples where one learner performs exceptionally well while another one is fairly bad.
Moreover, this might be useful for the construction of ensembles of learning algorithms.
Below, function [ggpairs](GGally::ggpairs) from package [%GGally] is used to generate a scatterplot
matrix of learner accuracies ([acc](&measures.md)) on the sonar data set.

```{r}
perf = getBMRPerformances(res, task.id = "Sonar-example", as.df = TRUE)
perfr = reshape(perf, direction = "wide", v.names = c("acc", "auc"), timevar = "learner.id",
  idvar = c("task.id", "iter"))
head(perfr)

GGally::ggpairs(perfr, c(3,5,7))
```


## Further comments

* In the examples shown in this section we applied "raw" learning algorithms, but often things
are more complicated.
At the very least, many learners have hyperparameters that need to be tuned to get sensible results.
Reliable performance estimates can be obtained by nested resampling, i.e., by doing the tuning in an
inner resampling loop while estimating the performance in an outer loop.
Moreover, you might want to combine learners with pre-processing steps like imputation, scaling,
outlier removal, dimensionality reduction or feature selection and so on.
All this can be easily done by using [%mlr]'s wrapper functionality.
The general principle is explained in the section about [wrapped learners](&wrapper.md) in the
Advanced part of this tutorial. There are also several sections devoted to common pre-processing
steps.
* Benchmark experiments can very quickly become computationally demanding. [%mlr] offers
some possibilities for [parallelization](parallelization.md).
