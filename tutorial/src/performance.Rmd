# Evaluating Learner Performance
The quality of the predictions of a model in [%mlr] can be assessed with respect to a
number of different performance measures.
In order to calculate the performance measures, call [&performance] on the object
returned by [predict](&predict.WrappedModel) and specify the desired performance measures.


## Available performance measures
[%mlr] provides a large number of performance measures for all types of learning problems.
Typical performance measures for *classification* are the mean misclassification error ([mmce](measures.md)),
accuracy ([acc](measures.md)) or measures based on [ROC analysis](roc_analysis.md).
For *regression* the mean of squared errors ([mse](measures.md)) or mean of absolute errors ([mae](measures.md))
are usually considered.
For *clustering* tasks, measures such as the Dunn index ([dunn](measures.md)) are provided,
while for *survival* predictions, the Concordance Index ([cindex](measures.md)) is
supported, and for *cost-sensitive* predictions the misclassification penalty
([mcp](measures.md)) and others. It is also possible to access the time to train the
learner ([timetrain](measures.md)), the time to compute the prediction ([timepredict](measures.md)) and their
sum ([timeboth](measures.md)) as performance measures.

To see which performance measures are implemented, have a look at the
[table of performance measures](measures.md) and the [&measures] documentation page.

If you want to implement an additional measure or include a measure with
non-standard misclassification costs, see the section on
[creating custom measures](create_measure.md).

## Listing measures
The properties and requirements of the individual measures are shown in the [table of performance measures](measures.md).

If you would like a list of available measures with certain properties or suitable for a
certain learning [&Task] use the function [&listMeasures].

```{r}
## Performance measures for classification with multiple classes
listMeasures("classif", properties = "classif.multi")
## Performance measure suitable for the iris classification task
listMeasures(iris.task)
```

## Calculate performance measures
In the following example we fit a [gradient boosting machine](&gbm::gbm) on a subset of the
[BostonHousing](&mlbench::BostonHousing) data set and calculate the mean squared error
([mse](measures.md)) on the remaining observations.

```{r}
n = getTaskSize(bh.task)
lrn = makeLearner("regr.gbm", n.trees = 1000)
mod = train(lrn, task = bh.task, subset = seq(1, n, 2))
pred = predict(mod, task = bh.task, subset = seq(2, n, 2))

# mse is the default measure for regression, we do not have to specify
# it here
performance(pred)
```

The following code computes the median of squared errors ([medse](measures.md)) instead.
```{r}
performance(pred, measures = medse)
```

Of course, we can also calculate multiple performance measures at once by simply passing a
list of measures which can also include [your own measure](create_measure.md).

Calculate the mean squared error, median squared error and mean absolute error ([mae](measures.md)).

```{r}
performance(pred, measures = list(mse, medse, mae))
```

For the other types of learning problems and measures, calculating the performance basically
works in the same way.


### Requirements of performance measures
Note that in order to calculate some performance measures it is required that you pass the
[&Task] or the [fitted model](&makeWrappedModel) in addition to the [&Prediction].

For example in order to assess the time needed for training ([timetrain](measures.md)), the fitted
model has to be passed.
```{r}
performance(pred, measures = timetrain, model = mod)
```

For many performance measures in cluster analysis the [&Task] is required.
```{r}
lrn = makeLearner("cluster.kmeans", centers = 3)
mod = train(lrn, mtcars.task)
pred = predict(mod, task = mtcars.task)

## Calculate the Dunn index
performance(pred, measures = dunn, task = mtcars.task)
```

Moreover, some measures require a certain type of prediction.
For example in binary classification in order to calculate the AUC ([auc](measures.md)) -- the area
under the ROC (receiver operating characteristic) curve -- we have to make sure that posterior
probabilities are predicted.
For more information on ROC analysis, see the section on [ROC analysis](roc_analysis.md).

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)
pred = predict(mod, task = sonar.task)

performance(pred, measures = auc)
```

Also bear in mind that many of the performance measures that are available for classification,
e.g., the false positive rate ([fpr](measures.md)), are only suitable for binary problems.


## Access a performance measure

Performance measures in [%mlr] are objects of class [Measure](&makeMeasure).
If you are interested in the properties or requirements of a single measure you can access it directly.
See the help page of [Measure](&makeMeasure) for information on the individual slots.

```{r}
## Mean misclassification error
str(mmce)
```

## Binary classification: Plot performance versus threshold

As you may recall (see the previous section on [making predictions](predict.md))
in binary classification we can adjust the threshold used to map probabilities to class labels.
Helpful in this regard is the function [&plotThreshVsPerf], which generates a plot of the
learner performance versus the threshold.

For more performance plots and automatic threshold tuning see [here](roc_analysis.md).

In the following example we consider the [Sonar](&mlbench::Sonar) data set and
plot the false positive rate ([fpr](measures.md)), the false negative rate ([fnr](measures.md))
as well as the misclassification rate ([mmce](measures.md)) for all possible threshold values.

```{r}
lrn = makeLearner("classif.lda", predict.type = "prob")
n = getTaskSize(sonar.task)
mod = train(lrn, task = sonar.task, subset = seq(1, n, by = 2))
pred = predict(mod, task = sonar.task, subset = seq(2, n, by = 2))

## Performance for the default threshold 0.5
performance(pred, measures = list(fpr, fnr, mmce))
## Plot false negative and positive rates as well as the error rate versus the threshold
d = generateThreshVsPerfData(pred, measures = list(fpr, fnr, mmce))
plotThreshVsPerf(d)
```

