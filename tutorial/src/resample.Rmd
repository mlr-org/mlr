```{r include=FALSE}
# Not strictly necessary, but otherwise we might get NAs later on
## if 'rpart' is not installed.
library("rpart")
```

# Resampling

In order to assess the performance of a learning algorithm, resampling
strategies are usually used. Resampling is a means of splitting the entire data
set into training and tet. There are various methods for doing this, for example
cross-validation and bootstrap, to mention just two popular approaches.
In **mlr**, the \man[resample] function, depending on the chosen resampling strategy,
fits the selected learner using the corresponding training sets and performs
predictions for the corresponding training/test sets and calculates the chosen
performance measures.


## Quick start

### Classification example


```{r}
library("mlr")

## Define a learning task and an appropriate learner
task = makeClassifTask(data = iris, target = "Species")
lrn = makeLearner("classif.lda")

## Perform a 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters = 3)
r = resample(lrn, task, rdesc)
r
```

In this example, the peformance measure is \man[mmce] (mean misclassification
error), the default for classification. See the documentation for
\man[measures] for a complete list of performance measures available in **mlr**.
More explanations, concerning the evaluation of learning methods, are given in
section [Evaluating Learner Performance](performance.md).


### Regression example

As an example with regression, we use the ``BostonHousing`` data set again.

```{r}
library("mlr")
library("mlbench")
data(BostonHousing)

## Define a learning task and an appropriate learner
task = makeRegrTask(data = BostonHousing, target = "medv")
lrn = makeLearner("regr.lm")

## Perform a 3-fold cross-validation
rdesc = makeResampleDesc("CV", iters=3)
r = resample(lrn, task, rdesc)
r
```

For regression, the default performance measure is \man[mse] (mean squared error).


## Further information

Resampling strategies concern the process of sampling new data sets that are
subsets of the main data set *D*. One wants to generate various training sets
that the learning method can fit models to and test sets that those models can
be evaluated on. We assume that every resampling strategy consists of a
number of iterations and each one defines a set of indices into *D* that determine
the respective training and test sets. These iterations are implemented by
storing the index set in a so called
\man2[ResampleInstance][makeResampleInstance] object. The reasons for having the
user create this data explicitly and not just set an option in an R function to
choose the resampling method are:

* It is easier to create paired experiments, where you train and test
  different methods on exactly the same sets, especially when you want
  to add another method to a comparison experiment you already did.
* It is easy to add other resampling methods later on. You can
  simply use S4 inheritance and derive from the \man2[ResampleInstance][makeResampleInstance]
  class, but you do not have to touch any methods that use the
  resampling strategy.

![Resampling Figure](../../_images/resampling.png "Resampling Figure")


## Resample descriptions and resample instances

There are two types of objects: a \man2[ResampleDesc][makeResampleDesc] object, which represents
a resample description, e.g. a 10-fold cross-validation, and a
\man2[ResampleInstance][makeResampleInstance]
object, which creates a resample instance for a specific task based on a resample
description. These can be generated by means of the factory methods \man[makeResampleDesc]
and \man[makeResampleInstance].

For every resampling strategy, there is a description class that inherits
from \man2[ResampleDesc][makeResampleDesc] and completely specifies the necessary
parameters, and a class inheriting from
\man2[ResampleInstance][makeResampleInstance].
This latter class takes the description object and performs the random
drawing of indices to separate the data into training and test. While this seems
overly complicated, it is necessary as sometimes one only wants to describe the
drawing process, while in other instances one wants to create the specific index
sets. There are convenient methods to make the construction process as
easy as possible.

```{r eval=FALSE}
## get the cv.instance directly
rdesc = makeResampleDesc("CV", iters = 10)
rinst  = makeResampleInstance(rdesc, size = nrow(iris))
```

Asking the \man2[ResampleDesc][makeResampleDesc] or
\man2[ResampleInstance][makeResampleInstance] objects for further information is
easy, just inspect the list elements by using the $ operator.

```{r eval=FALSE}
## description object
## number of iterations
rdesc$iters

## resample.instance object
## number of iterations
rinst$desc$iters

rinst$train.inds[[3]]
rinst$test.inds[[3]]
```

Please refer to the help pages of the specific classes for a complete
list of getters.


## Included resampling strategies

The package comes with a number of predefined strategies.

* 'Subsample' for subsampling,
* 'Holdout' for holdout (training/test),
* 'CV' for cross-validation,
* 'LOO' for leave-one-out,
* 'StratCV' for stratified cross-validation,
* 'RepCV' for repeated cross-validation,
* 'Bootstrap' for out-of-bag bootstrap.



### Subsampling

In each iteration *i* the data set *D* is randomly partitioned into a
training and a test set according to a given percentage, e.g. 2/3
training and 1/3 test set. If there is just one iteration, the strategy
is commonly called `holdout` or `test sample estimation`.

```{r eval=FALSE}
rdesc = makeResampleDesc("Subsample", iters = 10, split = 2/3)
rdesc = makeResampleDesc("Subsample", iters = 1, split = 2/3)
```

### k-fold cross-validation

The data set is partitioned into *k* subsets of (approximately) equal size.
In the *i*-th step of the *k* iterations, the *i*-th subset is
used as for testing, while the union of the remaining parts forms the training
set.

```{r eval=FALSE}
rdesc = makeResampleDesc("CV", iters = 10)
```

### Bootstrapping

*B* new data sets *D_1* to *D_B* are drawn from
*D* with replacement, each of the same size as *D*.
In the *i*-th iteration *D_i* forms the training set,
while the remaining elements from *D*, i.e. elements not
in the training set, form the test set.

```{r eval=FALSE}
rdesc = makeResampleDesc("Bootstrap", iters = 10)
```

<!--(
                     |resampling_desc_figure|

                     |resampling_nested_resampling_figure|
)-->

## Evaluation of Predictions after Resampling

The \man[resample] function evaluates the performance of your learner using
a certain resampling strategy for a given machine learning task.

When you use a resampling strategy, **mlr** offers the following
possibilities to evaluate your predictions. Every single resampling
iteration is handled as described in the explanation above, i.e. you
train a model on the training part of the data set, predict on the test set
and compare predicted and true labels to compute some performance
measure. This is done in every iteration so that you have
e.g. ten performance values in the case of 10-fold cross-validation.
The question arises of how to aggregate those values. You can specify
that explicitly, the default is to use the mean. Let's have a look at an
example.


### Classification example

For the example code, we use the standard ``iris`` data set and compare a
decision tree and the Linear Discriminant Analysis based on a 3-fold
cross-validation:

```{r}
## Classification task
task = makeClassifTask(data = iris, target = "Species")

## Resample instance for Cross-validation
rdesc = makeResampleDesc("CV", iters = 3)
rinst = makeResampleInstance(rdesc, task = task)
```

Now, we fit a decision tree for each fold and measure both the mean misclassification
error (\man[mmce]) and the accuracy (\man[acc]):

```{r}
## Merge learner (lrn), i.e. Decision Tree, classification task (task) and
## resample instance (rinst)
lrn = makeLearner("classif.rpart")
r1 = resample(lrn, task, rinst, list(mmce, acc))
```

Let's set a couple of hyperparameters for rpart:

```{r}
lrn1 = makeLearner("classif.rpart", minsplit = 10, cp = 0.03)
r1 = resample(lrn1, task, rinst, list(mmce, acc))


## Second resample for LDA as learner
lrn2 = makeLearner("classif.lda")
r2 = resample(lrn2, task, rinst, list(mmce, acc))

## Let's see how well both classifiers did w.r.t mean misclassification error and accuracy
r1[c("measures.test","aggr")]
r2[c("measures.test","aggr")]
```

To see the individual values for each fold on the test set, we can access
the `measures.test` element of the result list:

```{r}
r1$measures.test
```

As you can see above, every fold of the 3-fold cross-validation gives one mean
misclassification error (in the second column).

The aggregated performance values are stored in the `aggr` list element:

```{r}
r1$aggr
```

The latter value is the aggregation, i.e. by default the mean, of the three
misclassification errors from the table above.
Getting the single losses is of course possible as well.

### Regression example

Now, we use the ``BostonHousing`` data and compare the results of a neural net
and a k-nearest neighbour regression using out-of-bag bootstraping.

```{r}
library("mlr")
library("mlbench")
data(BostonHousing)

## Regression task
task = makeRegrTask(data = BostonHousing, target = "medv")

## Resample instance for bootstraping
rdesc = makeResampleDesc("Bootstrap", iters = 3)
rinst = makeResampleInstance(rdesc, task = task)

ms = list(mse, medae)

lrn1 = makeLearner("regr.nnet")
r1 = resample(lrn1, task, rinst, measures = ms)

## Another resampling for the k-Nearest Neighbor regression
lrn2 = makeLearner("regr.kknn")
r2 = resample(lrn2, task, rinst, measures = ms)
```

Now, we can compare both methods regarding mean squared error (\man[mse]) and the
median of absolute errors (\man[medae]):

```{r}
r1[c("measures.test","aggr")]
r2[c("measures.test","aggr")]
```
