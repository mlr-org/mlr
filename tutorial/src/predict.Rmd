# Predicting Outcomes for New Data
Predicting the target values for new observations is
implemented the same way as most of the other predict methods in **R**.
In general, all you need to do is call [predict](&predict.WrappedModel) on the object
returned by [&train] and pass the data you want predictions for.

There are two ways to pass the data:

* Either pass the [&Task] via the ``task`` argument or
* pass a [data frame](&base::data.frame) via the ``newdata`` argument.

The first way is preferable if you want predictions for data already included in a [&Task].

Just as [&train], the [predict](&predict.WrappedModel) function has a ``subset`` argument,
so you can set aside different portions of the data in [&Task] for training and prediction
(more advanced methods for splitting the data in train and test set are described in the
[section on resampling](resample.md)).

In the following example we fit a [gradient boosting machine](&gbm::gbm) to every second
observation of the [BostonHousing](&mlbench::BostonHousing) data set and make predictions
on the remaining data in [&bh.task].

```{r}
n = getTaskDescription(bh.task)$size
train.set = seq(1, n, by = 2)
test.set = seq(2, n, by = 2)
lrn = makeLearner("regr.gbm", n.trees = 100)
mod = train(lrn, bh.task, subset = train.set)

task.pred = predict(mod, task = bh.task, subset = test.set)
task.pred
```

The second way is useful if you want to predict data not included in the [&Task].

Here we cluster the ``iris`` data set without the target variable.
All observations with an odd index are included in the [&Task] and used for training.
Predictions are made for the remaining observations.

```{r}
n = nrow(iris)
iris.train = iris[seq(1, n, by = 2), -5]
iris.test = iris[seq(2, n, by = 2), -5]
task = makeClusterTask(data = iris.train)
mod = train("cluster.kmeans", task)

newdata.pred = predict(mod, newdata = iris.test)
newdata.pred
```

Note that for supervised learning you do not have to remove the target columns from the data.
These columns are automatically removed prior to calling the underlying ``predict`` method of the learner.


## Accessing the prediction
The returned [&Prediction] object is a named [list](&base::list). The most important element is ``data`` which is a
[data frame](&base::data.frame) that contains columns with the true values of the target variable (in case of
supervised learning problems) and the predictions.

In the following the predictions on the [BostonHousing](&mlbench::BostonHousing) and the
[iris](&datasets::iris) data sets are shown.
As you may recall, the predictions in the first case were made from a [&Task] and in the
second case from a [data frame](&base::data.frame).

```{r}
## Result of predict with data passed via task argument
head(task.pred$data)

## Result of predict with data passed via newdata argument
head(newdata.pred$data)
```

As you can see when predicting from a [&Task], the resulting [data frame](&base::data.frame)
contains an additional column, called ``id``, which tells us which element in the original data set
the prediction corresponds to.


### Extract Probabilities
The predicted probabilities can be extracted from the [&Prediction] using the function
[&getProbabilities].
Here is another cluster analysis example. We use [fuzzy c-means clustering](&e1071::cmeans)
on the [mtcars](&datasets::mtcars) data set.

```{r}
lrn = makeLearner("cluster.cmeans", predict.type = "prob")
mod = train(lrn, mtcars.task)

pred = predict(mod, task = mtcars.task)
head(getProbabilities(pred))
```

For classification problems there are some more things worth mentioning.
By default, class labels are predicted.

```{r}
## Linear discriminant analysis on the iris data set
mod = train("classif.lda", task = iris.task)

pred = predict(mod, task = iris.task)
pred
```

A confusion matrix can be obtained by calling [&getConfMatrix].
```{r}
getConfMatrix(pred)
```

In order to get predicted posterior probabilities we have to create a [Learner](&makeLearner)
with the appropriate ``predict.type``.

```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)

pred = predict(mod, newdata = iris)
head(pred$data)
```

In addition to the probabilities, class labels are predicted
by choosing the class with the maximum probability and breaking ties at random.

As mentioned above, the predicted posterior probabilities can be accessed via the
[&getProbabilities] function.
```{r}
head(getProbabilities(pred))
```


## Adjusting the threshold
We can set the threshold value that is used to map the predicted posterior probabilities to class labels.
Note that for this purpose we need to create a [Learner](&makeLearner) that predicts probabilities.
For binary classification, the threshold determines when the *positive* class is predicted.
The default is 0.5.
Now, we set the threshold for the positive class to 0.9 (that is, an example is assigned to the positive class if its posterior probability exceeds 0.9).
Which of the two classes is the positive one can be seen by accessing the [&Task].
To illustrate binary classification, we use the [Sonar](&mlbench::Sonar) data set from the [%mlbench] package.
```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, task = sonar.task)

## Label of the positive class
getTaskDescription(sonar.task)$positive

## Default threshold
pred1 = predict(mod, sonar.task)
pred1$threshold

## Set the threshold value for the positive class
pred2 = setThreshold(pred1, 0.9)
pred2$threshold
pred2

## We can also set the effect in the confusion matrix
getConfMatrix(pred1)
getConfMatrix(pred2)
```

Note that in the binary case [&getProbabilities] by default extracts the posterior
probabilities of the positive class only.
```{r}
head(getProbabilities(pred1))
## But we can change that, too
head(getProbabilities(pred1, cl = c("M", "R")))
```

It works similarly for multiclass classification.
The threshold has to be given by a named vector specifying the values by which each probability will be divided.
The class with the maximum resulting value is then selected.
```{r}
lrn = makeLearner("classif.rpart", predict.type = "prob")
mod = train(lrn, iris.task)
pred = predict(mod, newdata = iris)
pred$threshold
table(as.data.frame(pred)$response)
pred = setThreshold(pred, c(setosa = 0.01, versicolor = 50, virginica = 1))
pred$threshold
table(as.data.frame(pred)$response)
```

If you are interested in tuning the threshold (vector) have a look at the section about
[performance curves and threshold tuning](roc_analysis.md).


## Visualizing the prediction
The function [&plotLearnerPrediction] allows to visualize predictions, e.g., for teaching purposes
or exploring models.
It trains the chosen learning method for 1 or 2 selected features and then displays the
predictions with [ggplot](&ggplot2::ggplot).

For *classification*, we get a scatter plot of 2 features (by default the first 2 in the data set).
The type of symbol shows the true class labels of the data points.
The color indicates if observations are misclassified.
The posterior probabilities (if the learner under consideration supports this)
are represented by the background color.

The plot title displays the ID of the [Learner](&makeLearner) (in the following example CART),
its parameters, its training performance and its cross-validation performance.
[mmce](measures.md) stands for *mean misclassification error*, i.e., the error rate.
See the sections on [performance](performance.md) and
[resampling](resample.md) for further explanations.

```{r}
lrn = makeLearner("classif.rpart", id = "CART")
plotLearnerPrediction(lrn, task = iris.task)
```

For *clustering* we also get a scatter plot of two selected features.
The color of the points indicates the predicted cluster.

```{r}
lrn = makeLearner("cluster.SimpleKMeans")
plotLearnerPrediction(lrn, task = mtcars.task, features = c("disp", "drat"), cv = 0)
```

For *regression*, there are two types of plots.
The 1D plot shows the target values in relation to a single feature, the regression curve and,
if the chosen learner supports this, the estimated standard error.

```{r}
plotLearnerPrediction("regr.lm", features = "lstat", task = bh.task)
```

The 2D variant, as in the classification case, generates a scatter plot of 2 features.
The fill color of the dots illustrates the value of the target variable ``"medv"``, the
background colors show the estimated mean.
The plot does not represent the estimated standard error.

```{r}
plotLearnerPrediction("regr.lm", features = c("lstat", "rm"), task = bh.task)
```
