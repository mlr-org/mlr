# Tuning Hyperparameters

Many machine learning algorithms have hyperparameters that need to be set.
If selected by the user they can be specified as explained in the section about
[Learners](learner.md) -- simply pass them to [&makeLearner].
Often suitable parameter values are not obvious and it is preferable to tune the hyperparameters,
that is automatically identify values that lead to the best performance.


## Basics

For tuning you have to specify

* the search space,
* the optimization algorithm,
* an evaluation method, i.e., a resampling strategy and a performance measure.

The last point is already covered in this tutorial in the parts about the
[evaluation of learning methods](performance.md) and [resampling](resample.md).

Below we show how to specify the search space and optimization algorithm, how to do the
tuning and how to access the tuning result, using the example of a grid search.

Throughout this section we consider classification examples. For the other types of learning
problems tuning works analogously.


### Grid search with manual discretization

A grid search is one of the standard -- albeit slow -- ways to choose an
appropriate set of parameters from a given range of values.

We use the [iris classification task](&iris.task) for illustration and tune the
hyperparameters of an SVM (function [ksvm](&kernlab::ksvm) from the [%kernlab] package)
with a radial basis kernel.

First, we create a [ParamSet](&ParamHelpers::makeParamSet) object, which describes the
parameter space we wish to search.
This is done via function [makeParamSet](&ParamHelpers::makeParamSet).
We wish to tune the cost parameter `C` and the RBF kernel parameter `sigma` of the
[ksvm](&kernlab::ksvm) function.
Since we will use a grid search strategy, we add discrete parameters to the parameter set.
The specified `values` have to be vectors of feasible settings and the complete grid simply is
their cross-product.
Every entry in the parameter set has to be named according to the corresponding parameter
of the underlying **R** function.

Please note that whenever parameters in the underlying **R** functions should be
passed in a [list](&base::list) structure, [%mlr] tries to give you direct access to
each parameter and get rid of the list structure.
This is the case with the `kpar` argument of [ksvm](&kernlab::ksvm) which is a list of
kernel parameters like `sigma`.

```{r}
ps = makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
```

Additional to the parameter set, we need an instance of a [&TuneControl] object.
These describe the optimization strategy to be used and its settings.
Here we choose a grid search:

```{r}
ctrl = makeTuneControlGrid()
```

We will use 3-fold cross-validation to assess the quality of a specific parameter setting.
For this we need to create a resampling description just like in the [resampling](resample.md)
part of the tutorial.

```{r}
rdesc = makeResampleDesc("CV", iters = 3L)
```

Finally, by combining all the previous pieces, we can tune the SVM parameters by calling
[&tuneParams].

```{r}
res = tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps,
  control = ctrl)
res
```

[&tuneParams] simply performs the cross-validation for every element of the
cross-product and selects the parameter setting with the best mean performance.
As no performance measure was specified, by default the error rate ([mmce](&measures)) is
used.

Note that each [measure](&makeMeasure) "knows" if it is minimized or maximized during tuning.

```{r}
## error rate
mmce$minimize

## accuracy
acc$minimize
```

Of course, you can pass other measures and also a list of measures to [&tuneParams].
In the latter case the first measure is optimized during tuning, the others are simply evaluated.
If you are interested in optimizing several measures simultaneously have a look at the
paragraph about multi-criteria tuning below.

In the example below we calculate the accuracy ([acc](&measures)) instead of the error
rate.
We use function [&setAggregation], as described in the section on [resampling](resample.md),
to additionally obtain the standard deviation of the accuracy.

```{r}
res = tuneParams("classif.ksvm", task = iris.task, resampling = rdesc, par.set = ps,
  control = ctrl, measures = list(acc, setAggregation(acc, test.sd)), show.info = FALSE)
res
```


### Accessing the tuning result

The result object [&TuneResult] allows you to access the best found settings `$x` and their
estimated performance `$y`.

```{r}
res$x
res$y
```

Moreover, we can inspect all points evaluated during the search by accessing the
`$opt.path` (see also the documentation of [OptPath](&ParamHelpers::OptPath)).

```{r}
res$opt.path
opt.grid = as.data.frame(res$opt.path)
head(opt.grid)
```

A quick visualization of the performance values on the search grid can be accomplished as follows:

```{r tune_gridSearchVisualized}
library(ggplot2)
g = ggplot(opt.grid, aes(x = C, y = sigma, fill = acc.test.mean, label = round(acc.test.sd, 3)))
g + geom_tile() + geom_text(color = "white")
```

The colors of the tiles display the achieved accuracy, the tile labels show the standard deviation.


### Using the optimal parameter values

After tuning you can generate a [Learner](&makeLearner) with optimal hyperparameter settings
as follows:

```{r}
lrn = setHyperPars(makeLearner("classif.ksvm"), par.vals = res$x)
lrn
```

Then you can proceed as usual.
Here we refit and predict the learner on the complete [iris](&datasets::iris) data
set.

```{r}
m = train(lrn, iris.task)
predict(m, task = iris.task)
```


### Grid search without manual discretization

We can also specify the true numeric parameter types of `C` and `sigma` when creating the
parameter set and use the `resolution` option of [makeTuneControlGrid](&TuneControl) to
automatically discretize them.

Note how we also make use of the `trafo` option when creating the parameter set to easily
optimize on a log-scale.

Trafos work like this: All optimizers basically see the parameters on their
original scale (from -12 to 12) in this case and produce values on this scale during the search.
Right before they are passed to the learning algorithm, the transformation function is applied.

```{r}
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)
ctrl = makeTuneControlGrid(resolution = 3L)
rdesc = makeResampleDesc("CV", iters = 2L)
res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl)
res
```

Note that ``res$opt.path`` contains the parameter values *on the original scale*.

```{r}
as.data.frame(res$opt.path)
```

In order to get the *transformed* parameter values instead, use function
[trafoOptPath](&ParamHelpers::trafoOptPath).

```{r}
as.data.frame(trafoOptPath(res$opt.path))
```


## Iterated F-Racing for mixed spaces and dependencies

The package supports a larger number of tuning algorithms, which can all be looked up and
selected via [&TuneControl]. One of the cooler algorithms is iterated F-racing from the 
[%irace] package. This not only works for arbitrary parameter types (numeric, integer,
discrete, logical), but also for so-called dependent / hierarchical parameters:

```{r}
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x,
    requires = quote(kernel == "rbfdot")),
  makeIntegerParam("degree", lower = 2L, upper = 5L,
    requires = quote(kernel == "polydot"))
)
ctrl = makeTuneControlIrace(maxExperiments = 200L)
rdesc = makeResampleDesc("Holdout")
res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
print(head(as.data.frame(res$opt.path)))
```

See how we made the kernel parameters like `sigma` and `degree` dependent on the `kernel`
selection parameters? This approach allows you to tune parameters of multiple kernels at once, 
efficiently concentrating on the ones which work best for your given data set.


## Tuning across whole model spaces with ModelMultiplexer

We can now take the following example even one step further. If we use the
[ModelMultiplexer](&makeModelMultiplexer) we can tune over different model classes at once,
just as we did with the SVM kernels above.

```{r}
base.learners = list(
  makeLearner("classif.ksvm"),
  makeLearner("classif.randomForest")
)
lrn = makeModelMultiplexer(base.learners)
```

Function [&makeModelMultiplexerParamSet] offers a simple way to contruct parameter set for tuning:
The parameter names are prefixed automatically and the `requires` element is set, too,
to make all paramaters subordinate to `selected.learner`.

```{r}
ps = makeModelMultiplexerParamSet(lrn,
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeIntegerParam("ntree", lower = 1L, upper = 500L)
)
print(ps)
rdesc = makeResampleDesc("CV", iters = 2L)
ctrl = makeTuneControlIrace(maxExperiments = 200L)
res = tuneParams(lrn, iris.task, rdesc, par.set = ps, control = ctrl, show.info = FALSE)
print(head(as.data.frame(res$opt.path)))
```


## Nested resampling

As we continually optimize over the same data during tuning, the estimated
performance value `res$y` might be optimistically biased.
A clean approach to ensure unbiased performance estimation is nested resampling,
where we embed the whole model selection process into an outer resampling loop.

Actually, we can get this for free without programming any looping by simply using the
[wrapper functionality](wrapper.md) of [%mlr].
See also function [&makeTuneWrapper].

Let's use cross-validation with 5 folds in the outer loop and use simple
holdout test set estimation during tuning in the inner loop.

```{r}
ps = makeParamSet(
  makeDiscreteParam("C", values = 2^(-2:2)),
  makeDiscreteParam("sigma", values = 2^(-2:2))
)
ctrl = makeTuneControlGrid()
inner = makeResampleDesc("Holdout")
outer = makeResampleDesc("CV", iters = 5)
lrn = makeTuneWrapper("classif.ksvm", inner, par.set = ps, control = ctrl, show.info = FALSE)
r = resample(lrn, iris.task, resampling = outer, extract = getTuneResult, show.info = FALSE)
```

If we want to find out how good those configurations are on the entire data
set, we can look at the measures that we already know from [resampling](resample.md).
We receive 5 misclassification errors (one for each optimal parameter
configuration per outer fold) and one aggregated version, i.e., the mean,
of those 5 values.

```{r}
r$measures.test
r$aggr
```


### Accessing the tuning result

We have kept the results of the tuning for further evaluations.
For example one might want to find out, if the best obtained configurations vary for the
different outer splits.
As storing entire models may be expensive we used the `extract` option of [&resample].
Function [&getTuneResult] returns the optimal parameter values and the optimization path
for each iteration of the outer resampling loop.

```{r}
r$extract
```
We can compare the optimal parameter settings obtained in the 5 resampling iterations.
As you can see, the optimal configuration usually depends on the data. You may
be able to identify a *range* of parameter settings that achieve good
performance though, e.g., the values for `C` should be at least 1 and the values
for `sigma` should be between 0 and 1.

In the following we extract the `opt.path`'s for each of the 5 cross-validation iterations.

```{r}
opt.paths = lapply(r$extract, function(x) as.data.frame(x$opt.path))
head(opt.paths[[1]])
```

With the following code you can visualize the `opt.path` of the first resampling iteration.

```{r tune_nestedGridSearchVisualized}
opt.mmce = lapply(opt.paths, function(x) x$mmce.test.mean)
opt.grid = opt.paths[[1]][, 1:2]
opt.grid$mmce.test.mean = apply(simplify2array(opt.mmce), 1, mean)
g = ggplot(opt.grid, aes(x = C, y = sigma, fill = mmce.test.mean))
g + geom_tile()
```


## Multi-criteria evaluation and optimization

During tuning you might want to optimize multiple, potentially conflicting, performance measures
simultaneously.

In the following example we aim to minimize both, the false positive and the false negative rates
([fpr](&measures) and [fnr](&measures)).
We again tune the hyperparameters of an SVM (function [ksvm](&kernlab::ksvm)) with a radial
basis kernel and use the [sonar classification task](&sonar.task) for illustration.
As search strategy we choose a random search.

For all available multi-criteria tuning algorithms see [&TuneMultiCritControl].

```{r}
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)
ctrl = makeTuneMultiCritControlRandom(maxit = 30L)
rdesc = makeResampleDesc("Holdout")
res = tuneParamsMultiCrit("classif.ksvm", task = sonar.task, resampling = rdesc, par.set = ps,
  measures = list(fpr, fnr), control = ctrl, show.info = FALSE)
res
head(as.data.frame(trafoOptPath(res$opt.path)))
```

The results can be visualized with function [&plotTuneMultiCritResult].
The plot shows the false positive and false negative rates for all parameter settings evaluated
during tuning. Points on the Pareto front are slightly increased.

```{r}
plotTuneMultiCritResult(res)
```


## Further comments

* Tuning works for all other tasks like regression, survival analysis and so on in a completely
  similar fashion.

* In longer running tuning experiments it is very annoying if the computation stops due to
  numerical or other errors. Have a look at `on.learner.error` in [&configureMlr] as well as
  the examples given in section [Configure mlr](configureMlr.md) of this tutorial.
  You might also want to inform yourself about `impute.val` in [&TuneControl].

<!--(
.. |tune-varsel_processing| image:: /_images/tune-varsel_processing.png
     :align: middle
     :width: 50em
     :alt: Variable selection as a tuning.
     
)-->
