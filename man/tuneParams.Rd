% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tuneParams.R
\name{tuneParams}
\alias{tuneParams}
\title{Hyperparameter tuning.}
\usage{
tuneParams(
  learner,
  task,
  resampling,
  measures,
  par.set,
  control,
  show.info = getMlrOption("show.info"),
  resample.fun = resample
)
}
\arguments{
\item{learner}{(\link{Learner} | \code{character(1)})\cr
The learner.
If you pass a string the learner will be created via \link{makeLearner}.}

\item{task}{(\link{Task})\cr
The task.}

\item{resampling}{(\link{ResampleInstance} | \link{ResampleDesc})\cr
Resampling strategy to evaluate points in hyperparameter space. If you pass a description,
it is instantiated once at the beginning by default, so all points are
evaluated on the same training/test sets.
If you want to change that behavior, look at \link{TuneControl}.}

\item{measures}{(list of \link{Measure} | \link{Measure})\cr
Performance measures to evaluate. The first measure, aggregated by the first aggregation function
is optimized, others are simply evaluated.
Default is the default measure for the task, see here \link{getDefaultMeasure}.}

\item{par.set}{(\link[ParamHelpers:makeParamSet]{ParamHelpers::ParamSet})\cr
Collection of parameters and their constraints for optimization.
Dependent parameters with a \code{requires} field must use \code{quote} and not
\code{expression} to define it.}

\item{control}{(\link{TuneControl})\cr
Control object for search method. Also selects the optimization algorithm for tuning.}

\item{show.info}{(\code{logical(1)})\cr
Print verbose output on console?
Default is set via \link{configureMlr}.}

\item{resample.fun}{(\link{closure})\cr
The function to use for resampling. Defaults to \link{resample}. If a user-given function
is to be used instead, it should take the arguments \dQuote{learner}, \dQuote{task}, \dQuote{resampling},
\dQuote{measures}, and \dQuote{show.info}; see \link{resample}. Within this function,
it is easiest to call \link{resample} and possibly modify the result.
However, it is possible to return a list with only the following essential slots:
the \dQuote{aggr} slot for general tuning, additionally the \dQuote{pred} slot if threshold tuning is performed
(see \link{TuneControl}), and the \dQuote{err.msgs} and \dQuote{err.dumps} slots for error reporting.
This parameter must be the default when \code{mbo} tuning is performed.}
}
\value{
(\link{TuneResult}).
}
\description{
Optimizes the hyperparameters of a learner.
Allows for different optimization methods, such as grid search, evolutionary strategies,
iterated F-race, etc. You can select such an algorithm (and its settings)
by passing a corresponding control object. For a complete list of implemented algorithms look at
\link{TuneControl}.

Multi-criteria tuning can be done with \link{tuneParamsMultiCrit}.
}
\note{
If you would like to include results from the training data set, make
sure to appropriately adjust the resampling strategy and the aggregation for
the measure. See example code below.
}
\examples{
\dontshow{ if (requireNamespace("kernlab")) \{ }
\dontshow{ if (requireNamespace("kernlab")) \{ }
\dontshow{ if (requireNamespace("irace")) \{ }
set.seed(123)
# a grid search for an SVM (with a tiny number of points...)
# note how easily we can optimize on a log-scale
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x)
)
ctrl = makeTuneControlGrid(resolution = 2L)
rdesc = makeResampleDesc("CV", iters = 2L)
res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl)
print(res)
# access data for all evaluated points
df = as.data.frame(res$opt.path)
df1 = as.data.frame(res$opt.path, trafo = TRUE)
print(head(df[, -ncol(df)]))
print(head(df1[, -ncol(df)]))
# access data for all evaluated points - alternative
df2 = generateHyperParsEffectData(res)
df3 = generateHyperParsEffectData(res, trafo = TRUE)
print(head(df2$data[, -ncol(df2$data)]))
print(head(df3$data[, -ncol(df3$data)]))
\dontrun{
# we optimize the SVM over 3 kernels simultanously
# note how we use dependent params (requires = ...) and iterated F-racing here
ps = makeParamSet(
  makeNumericParam("C", lower = -12, upper = 12, trafo = function(x) 2^x),
  makeDiscreteParam("kernel", values = c("vanilladot", "polydot", "rbfdot")),
  makeNumericParam("sigma", lower = -12, upper = 12, trafo = function(x) 2^x,
    requires = quote(kernel == "rbfdot")),
  makeIntegerParam("degree", lower = 2L, upper = 5L,
    requires = quote(kernel == "polydot"))
)
print(ps)
ctrl = makeTuneControlIrace(maxExperiments = 5, nbIterations = 1, minNbSurvival = 1)
rdesc = makeResampleDesc("Holdout")
res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps, control = ctrl)
print(res)
df = as.data.frame(res$opt.path)
print(head(df[, -ncol(df)]))

# include the training set performance as well
rdesc = makeResampleDesc("Holdout", predict = "both")
res = tuneParams("classif.ksvm", iris.task, rdesc, par.set = ps,
  control = ctrl, measures = list(mmce, setAggregation(mmce, train.mean)))
print(res)
df2 = as.data.frame(res$opt.path)
print(head(df2[, -ncol(df2)]))
}
\dontshow{ \} }
\dontshow{ \} }
\dontshow{ \} }
}
\seealso{
\link{generateHyperParsEffectData}

Other tune: 
\code{\link{TuneControl}},
\code{\link{getNestedTuneResultsOptPathDf}()},
\code{\link{getNestedTuneResultsX}()},
\code{\link{getResamplingIndices}()},
\code{\link{getTuneResult}()},
\code{\link{makeModelMultiplexer}()},
\code{\link{makeModelMultiplexerParamSet}()},
\code{\link{makeTuneControlCMAES}()},
\code{\link{makeTuneControlDesign}()},
\code{\link{makeTuneControlGenSA}()},
\code{\link{makeTuneControlGrid}()},
\code{\link{makeTuneControlIrace}()},
\code{\link{makeTuneControlMBO}()},
\code{\link{makeTuneControlRandom}()},
\code{\link{makeTuneWrapper}()},
\code{\link{tuneThreshold}()}
}
\concept{tune}
