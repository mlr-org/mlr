% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StackedLearner.R
\name{makeStackedLearner}
\alias{makeStackedLearner}
\title{Create a stacked learner object.}
\usage{
makeStackedLearner(
  base.learners,
  super.learner = NULL,
  predict.type = NULL,
  method = "stack.nocv",
  use.feat = FALSE,
  resampling = NULL,
  parset = list()
)
}
\arguments{
\item{base.learners}{[(list of) [Learner])\cr
A list of learners created with `makeLearner`.}

\item{super.learner}{[\code{\link{Learner} | character(1)}]\cr
The super learner that makes the final prediction based on the base learners.
If you pass a string, the super learner will be created via `makeLearner`.
Not used for `method = 'average'`. Default is `NULL`.}

\item{predict.type}{(`character(1)`)\cr
Sets the type of the final prediction for `method = 'average'`.
For other methods, the predict type should be set within `super.learner`.
If the type of the base learner prediction, which is set up within `base.learners`, is
\describe{
 \item{`"prob"`}{then `predict.type = 'prob'` will use the average of all
 bease learner predictions and `predict.type = 'response'` will use
 the class with highest probability as final prediction.}
 \item{`"response"`}{then, for classification tasks with `predict.type = 'prob'`,
 the final prediction will be the relative frequency based on the predicted base learner classes
 and classification tasks with `predict.type = 'response'` will use majority vote of the base
 learner predictions to determine the final prediction.
 For regression tasks, the final prediction will be the average of the base learner predictions.}
}}

\item{method}{(`character(1)`)\cr
\dQuote{average} for averaging the predictions of the base learners,
\dQuote{stack.nocv} for building a super learner using the predictions of the base learners,
\dQuote{stack.cv} for building a super learner using crossvalidated predictions of the base learners.
\dQuote{hill.climb} for averaging the predictions of the base learners, with the weights learned from
hill climbing algorithm and
\dQuote{compress} for compressing the model to mimic the predictions of a collection of base learners
while speeding up the predictions and reducing the size of the model.
Default is \dQuote{stack.nocv},}

\item{use.feat}{(`logical(1)`)\cr
Whether the original features should also be passed to the super learner.
Not used for `method = 'average'`.
Default is `FALSE`.}

\item{resampling}{([ResampleDesc])\cr
Resampling strategy for `method = 'stack.cv'`.
Currently only CV is allowed for resampling.
The default `NULL` uses 5-fold CV.}

\item{parset}{the parameters for `hill.climb` method, including
\describe{
  \item{`replace`}{Whether a base learner can be selected more than once.}
  \item{`init`}{Number of best models being included before the selection algorithm.}
  \item{`bagprob`}{The proportion of models being considered in one round of selection.}
  \item{`bagtime`}{The number of rounds of the bagging selection.}
  \item{`metric`}{The result evaluation metric function taking two parameters `pred` and `true`,
  the smaller the score the better.}
}
the parameters for `compress` method, including
\describe{
   \item{k}{the size multiplier of the generated data}
   \item{prob}{the probability to exchange values}
   \item{s}{the standard deviation of each numerical feature}
}}
}
\description{
A stacked learner uses predictions of several base learners and fits
a super learner using these predictions as features in order to predict the outcome.
The following stacking methods are available:

 \describe{
  \item{`average`}{Averaging of base learner predictions without weights.}
  \item{`stack.nocv`}{Fits the super learner, where in-sample predictions of the base learners are used.}
  \item{`stack.cv`}{Fits the super learner, where the base learner predictions are computed
  by crossvalidated predictions (the resampling strategy can be set via the `resampling` argument).}
  \item{`hill.climb`}{Select a subset of base learner predictions by hill climbing algorithm.}
  \item{`compress`}{Train a neural network to compress the model from a collection of base learners.}
 }
}
\examples{
# Classification
data(iris)
tsk = makeClassifTask(data = iris, target = "Species")
base = c("classif.rpart", "classif.lda", "classif.svm")
lrns = lapply(base, makeLearner)
lrns = lapply(lrns, setPredictType, "prob")
m = makeStackedLearner(base.learners = lrns,
  predict.type = "prob", method = "hill.climb")
tmp = train(m, tsk)
res = predict(tmp, tsk)

# Regression
data(BostonHousing, package = "mlbench")
tsk = makeRegrTask(data = BostonHousing, target = "medv")
base = c("regr.rpart", "regr.svm")
lrns = lapply(base, makeLearner)
m = makeStackedLearner(base.learners = lrns,
  predict.type = "response", method = "compress")
tmp = train(m, tsk)
res = predict(tmp, tsk)
}
